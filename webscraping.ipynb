{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping Financial Data - Upwork listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "\n",
    "The client has requested data to be scraped from various markets and placed into an excel table.  The links to all requested markets have been included as part of an excel spreadsheet.\n",
    "\n",
    "## Preliminary Thought process\n",
    "\n",
    "Data Collection:\n",
    "* Parse all hyperlinks from excel spreadsheet\n",
    "\n",
    "* Webscrape relevant values from hyperlink\n",
    "\n",
    "* Create function to loop through and repeat for each link\n",
    "\n",
    "Data Cleaning and Input:\n",
    "* Take numbers scraped from web pages\n",
    "\n",
    "* Input to relevant cells\n",
    "\n",
    "* Format accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tools required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant information to be webscraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of things we need:\n",
    "    * Profit - done\n",
    "    * Balance - done\n",
    "    * Equity - done (/x80 is a Euro sign)\n",
    "    * Deposits - done\n",
    "    * Withdrawals - done\n",
    "    * Trades - done\n",
    "    * Won(%) - done\n",
    "    * Av. Trade Time - done\n",
    "    * Profit Factor - done\n",
    "    * Daily - done\n",
    "    * Monthly - done\n",
    "    * Trades per month - done\n",
    "    * Expectancy - done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing hyperlinks from excel spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "wb = openpyxl.load_workbook(# path to spreadsheet)\n",
    "ws = wb['Sheet1']\n",
    "\n",
    "# Select columns\n",
    "hyperlinks = ws[\"B\"]\n",
    "\n",
    "# Extract hyperlinks starting from row 2, column 2\n",
    "hyperlinks_list= []\n",
    "market_list = []\n",
    "for num in range(len(hyperlinks)-1):\n",
    "    link = (ws.cell(row=num+2, column=2))\n",
    "    \n",
    "    # Create list of market names\n",
    "    market = link.value\n",
    "    market_list.append(str(market))\n",
    "    \n",
    "    # Convert strings into hyperlinks and append to list\n",
    "    hyperlink = link.hyperlink.target\n",
    "    hyperlinks_list.append(hyperlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(market_list[0]), print(hyperlinks_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping data from tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create soup object\n",
    "link = hyperlinks_list[0]\n",
    "url = get(link)\n",
    "soup = BeautifulSoup(url.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape relevant data and convert to text\n",
    "table = soup.find('div', class_=\"tab-pane active\")\n",
    "table_text = table.text.strip()\n",
    "print(\" \".join(table_text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to scrape for all markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def table_scraper(markets, hyperlinks):\n",
    "    \"\"\"\n",
    "    Function used to loop over markets and hyperlinks to markets and extract the desired tables.\n",
    "    \"\"\"\n",
    "    for m, h in zip(markets, hyperlinks):\n",
    "        print(f'Getting data for {m}...')\n",
    "        url = get(h)\n",
    "        soup = BeautifulSoup(url.text, 'lxml')\n",
    "        \n",
    "        table = soup.findAll(\"li\", {\"class\":\"list-group-item\"})\n",
    "        \n",
    "        # Profits\n",
    "        pr = soup.findAll(\"div\", {\"class\": \"number\"})\n",
    "        pro = pr[2].text.strip()\n",
    "        profit = pro.encode('ascii', errors='ignore')\n",
    "        \n",
    "        \n",
    "        # Balance\n",
    "        ba = soup.find(\"li\", class_=\"list-group-item\")\n",
    "        bal = ba.text.strip().split()\n",
    "        balance = bal[1].encode('ascii', errors='ignore')\n",
    "        \n",
    "        \n",
    "        # Equity\n",
    "        eq = table[1].text.split()\n",
    "        equity = eq[2].encode('ascii', errors='ignore')\n",
    "        \n",
    "        \n",
    "        # Deposits\n",
    "        de = table[3].text.split()\n",
    "        deposits = de[1].encode('ascii', errors='ignore')\n",
    "        \n",
    "        \n",
    "        # Withdrawals\n",
    "        wi=table[4].text.strip().split()\n",
    "        withdrawals=wi[1].encode('ascii', errors='ignore')\n",
    "        #print\n",
    "        \n",
    "        # Trades\n",
    "        tr = table[5].text.split()\n",
    "        trades = tr[1].encode('ascii', errors='ignore')\n",
    "        \n",
    "        \n",
    "        # Won\n",
    "        wo = table[7].text.strip().split()\n",
    "        won = wo[1].encode('ascii', errors='ignore')\n",
    "      \n",
    "        \n",
    "        # Average trade time\n",
    "        avg_trade_time = table[8].text.strip().split()\n",
    "        att = ' '.join(avg_trade_time[3:5])\n",
    "          \n",
    "        \n",
    "        # Profit Factor\n",
    "        pf = table[10].text.strip().split()\n",
    "        profit_factor = pf[2].encode('ascii', errors='ignore')\n",
    "        \n",
    "    \n",
    "        # Daily\n",
    "        da = table[11].text.strip().split()\n",
    "        daily = da[1].encode('ascii', errors='ignore')\n",
    "        \n",
    "        \n",
    "        # Monthly\n",
    "        mo = table[12].text.strip().split()\n",
    "        monthly = mo[1].encode('ascii', errors='ignore')\n",
    "        \n",
    "        \n",
    "        # Trades per month\n",
    "        tpm_ = table[13].text.strip().split()\n",
    "        tpm = tpm_[3].encode('ascii', errors='ignore')\n",
    "      \n",
    "        \n",
    "        # Expectancy\n",
    "        ex = table[14].text.strip().split()\n",
    "        expectancy = ex[4].encode('ascii', errors='ignore')\n",
    "        \n",
    "        #print(\"\\n\")\n",
    "        print(f'Writing data for {m}...')\n",
    "        database = open(os.path.join(os.path.expanduser(\n",
    "        '~'), 'Desktop', 'webscraped-info.txt'), 'a', newline='')\n",
    "        database.write(f\"{m};{profit};{balance};{equity};{deposits};{withdrawals};{trades};{won};{att};{profit_factor};{daily};{monthly};{tpm};{expectancy}\\n\")\n",
    "        print(f\"{m} completed!\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print('All completed!')\n",
    "    \n",
    "table_scraper(market_list, hyperlinks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems encountered:\n",
    "\n",
    "* Note to self: be careful when webscraping.  Incorrect list slicing of web elements lead to job taking a lot longer than it should have.\n",
    "\n",
    "* Loops in loops = no bueno.  Running `for x,y in zip(list_a, list_b)` was the answer.\n",
    "\n",
    "* Financial data can include currency symbols which Jupyter doesn't like.  Solved by using `encode()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing thoughts:\n",
    "\n",
    "* Plan did not go as thought.  Instead, I wrote to a text file which I could import into Excel because it would be significantly quicker to copy and paste instead of import it via python.\n",
    "\n",
    "* Ended up taking the output from the text file, importing into Excel, and editing there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
